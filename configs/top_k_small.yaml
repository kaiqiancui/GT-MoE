# Configuration for Top-K Gating Custom Model (Aligned with RD-ESI Debug Scale)

# Model configuration
model:
  vocab_size: 50257                         # GPT-2 vocabulary size - Kept same
  hidden_size: 256                          # MODIFIED: Was 512, matched RD-ESI
  num_layers: 4                             # MODIFIED: Was 6, matched RD-ESI
  num_heads: 4                              # MODIFIED: Was 8, matched RD-ESI
  intermediate_size: 1024                   # MODIFIED: Was 2048, matched RD-ESI (256 * 4)
  max_seq_len: 1024                         # Kept same
  dropout: 0.1                              # Kept same
  
  # MoE configuration
  moe_layers: [1, 3]                        # MODIFIED: Was [2, 4], adjusted for new num_layers to match RD-ESI's 2 MoE layers
  moe_config:
    num_experts: 8                          # MODIFIED: Was 16, matched RD-ESI
    top_k: 2                                # Kept same
    router_config:
      # No RD-ESI specific parameters for Top-K
      # Using standard Top-K gating with auxiliary loss
      use_aux_loss: true
      aux_loss_weight: 0.01                 # Kept same, matches RD-ESI's aux_loss_coef
    expert_dropout: 0.1                     # Kept same

# Tokenizer configuration
tokenizer:
  tokenizer_name_or_path: "gpt2"            # Kept same
  use_fast: true                            # Kept same
  add_special_tokens: true                  # Kept same
  padding_side: "right"                     # Kept same

# Data configuration
# Note: RD-ESI config has C4-specific loading (num_files_to_load, file_pattern). 
# Ensure your data pipeline for Top-K provides a comparable dataset size if not using the exact same C4 loading.
data:
  batch_size: 16                            # Kept same
  max_length: 512                           # Kept same
  num_workers: 8                            # MODIFIED: Was 4, matched RD-ESI
  streaming: false                          # Kept same, RD-ESI C4 loading is likely non-streaming for this setup

# Trainer configuration
trainer:
  output_dir: "results/top_k_run_scaled_debug" # MODIFIED: Suggested new output directory
  device: "cuda:2"                          # MODIFIED: Was "cuda", matched RD-ESI specific device
  optimizer: "adamw"                        # Kept same
  learning_rate: 3e-4                       # MODIFIED: Was 5e-5, matched RD-ESI
  weight_decay: 0.01                        # Kept same
  lr_scheduler: "cosine"                    # Kept same
  warmup_steps: 500                         # MODIFIED: Was 100, matched RD-ESI
  max_steps: 100005                         # MODIFIED: Was 5000, matched RD-ESI
  gradient_accumulation_steps: 2            # MODIFIED: Was 4, matched RD-ESI
  max_grad_norm: 1.0                        # Kept same
  log_interval: 10                          # Kept same
  eval_interval: 2500                       # MODIFIED: Was 500, matched RD-ESI
  save_interval: 5000                       # MODIFIED: Was 1000, matched RD-ESI
  use_wandb: false                          # Kept same
  wandb_project: "rd-esi-moe"               # Kept same
  wandb_run_name: "top-k-run-scaled-debug"  # MODIFIED: Suggested new run name

  # Performance optimization options (matched from RD-ESI)
  use_amp: true                             # ADDED: Matched RD-ESI
  use_checkpoint: true                      # ADDED: Matched RD-ESI
  cuda_deterministic: false                 # ADDED: Matched RD-ESI
  benchmark_cudnn: true                     # ADDED: Matched RD-ESI
  
  # Memory optimization (matched from RD-ESI)
  optimizer_states_on_cpu: true             # ADDED: Matched RD-ESI
  save_only_last: true                      # ADDED: Matched RD-ESI
  save_optimizer: true                      # ADDED: Assuming this should also match

# Evaluator configuration
evaluator:
  output_dir: "results/top_k_evaluation_scaled_debug" # MODIFIED: Suggested new output directory
  device: "cuda:2"                          # MODIFIED: Was "cuda", matched RD-ESI specific device