# Configuration for Top-K Gating Custom Model

# Model configuration
model:
  vocab_size: 50257  # GPT-2 vocabulary size
  hidden_size: 512
  num_layers: 6
  num_heads: 8
  intermediate_size: 2048
  max_seq_len: 1024
  dropout: 0.1
  
  # MoE configuration
  moe_layers: [2, 4]  # Layers that use MoE
  moe_config:
    num_experts: 16
    top_k: 2
    router_config:
      # No RD-ESI specific parameters for Top-K
      # Using standard Top-K gating with auxiliary loss
      use_aux_loss: true
      aux_loss_weight: 0.01
    expert_dropout: 0.1

# Tokenizer configuration
tokenizer:
  tokenizer_name_or_path: "gpt2"
  use_fast: true
  add_special_tokens: true
  padding_side: "right"

# Data configuration
data:
  batch_size: 16
  max_length: 512
  num_workers: 4
  streaming: false

# Trainer configuration
trainer:
  output_dir: "results/top_k_run"
  device: "cuda"  # or "cpu" if no GPU available
  optimizer: "adamw"
  learning_rate: 5e-5
  weight_decay: 0.01
  lr_scheduler: "cosine"
  warmup_steps: 100
  max_steps: 5000
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  log_interval: 10
  eval_interval: 500
  save_interval: 1000
  use_wandb: false
  wandb_project: "rd-esi-moe"
  wandb_run_name: "top-k-run"

# Evaluator configuration
evaluator:
  output_dir: "results/top_k_evaluation"
  device: "cuda"  # or "cpu" if no GPU available
