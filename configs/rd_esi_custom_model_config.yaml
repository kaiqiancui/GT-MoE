# Configuration for RD-ESI Custom Model

# Model configuration
model:
  vocab_size: 50257  # GPT-2 vocabulary size
  hidden_size: 512
  num_layers: 6
  num_heads: 8
  intermediate_size: 2048
  max_seq_len: 1024
  dropout: 0.1
  
  # MoE configuration
  moe_layers: [2, 4]  # Layers that use MoE
  moe_config:
    num_experts: 16
    top_k: 2
    router_config:
      beta: 0.1  # Weight for reputation score
      gamma: 0.1  # Weight for load penalty
      alpha: 0.9  # Smoothing factor for reputation EMA updates
      use_exploration_bonus: true
      exploration_c: 0.1  # Constant for UCB exploration bonus
      use_reputation_decay: true
      decay_rate: 0.99  # Rate at which reputation decays
      load_ema_alpha: 0.9  # Smoothing factor for load EMA updates
    expert_dropout: 0.1

# Tokenizer configuration
tokenizer:
  tokenizer_name_or_path: "gpt2"
  use_fast: true
  add_special_tokens: true
  padding_side: "right"

# Data configuration
data:
  batch_size: 16
  max_length: 512
  num_workers: 4
  streaming: false

# Trainer configuration
trainer:
  output_dir: "results/rd_esi_run"
  device: "cuda"  # or "cpu" if no GPU available
  optimizer: "adamw"
  learning_rate: 5e-5
  weight_decay: 0.01
  lr_scheduler: "cosine"
  warmup_steps: 100
  max_steps: 5000
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  log_interval: 10
  eval_interval: 500
  save_interval: 1000
  use_wandb: false
  wandb_project: "rd-esi-moe"
  wandb_run_name: "rd-esi-run"

# Evaluator configuration
evaluator:
  output_dir: "results/rd_esi_evaluation"
  device: "cuda"  # or "cpu" if no GPU available
