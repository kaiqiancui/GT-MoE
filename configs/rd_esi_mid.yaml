# config_plan_B_balanced.yaml
model:
  vocab_size: 50257
  hidden_size: 512              # 一个小而有效的尺寸
  num_layers: 6                 # 6层是小型transformer的经典配置
  num_heads: 8
  intermediate_size: 2048       # (512 * 4)
  max_seq_len: 1024
  dropout: 0.1
  
  # MoE configuration
  moe_layers: [2, 4]            # 在6层模型中的位置
  moe_config:
    num_experts: 16             # 专家数量适中
    top_k: 2
    router_config:
      beta: 0.1
      gamma: 0.1
      alpha: 0.9
      use_exploration_bonus: true
      exploration_c: 0.1
      use_reputation_decay: true
      decay_rate: 0.99
      load_ema_alpha: 0.9
    expert_dropout: 0.1

# Tokenizer configuration
tokenizer:
  tokenizer_name_or_path: "gpt2"
  use_fast: true
  add_special_tokens: true
  padding_side: "right"

# Data configuration
data:
  batch_size: 16                # 平衡内存和速度
  max_length: 512               # 标准的序列长度
  num_workers: 4
  streaming: false

# Trainer configuration
trainer:
  output_dir: "results/rd_esi_balanced_run"
  device: "cuda:3"
  optimizer: "adamw"
  learning_rate: 1.5e-4         # 一个稳健的、适用于中小型模型从头训练的学习率
  weight_decay: 0.01
  lr_scheduler: "cosine"
  warmup_steps: 2000            # 2k步预热比较稳妥
  max_steps: 40000              # 4万步足以让小模型学习到很多东西
  gradient_accumulation_steps: 4 # 保持一定的有效批量大小
  max_grad_norm: 1.0
  log_interval: 10
  eval_interval: 500
  save_interval: 1000
  use_wandb: false
  wandb_project: "rd-esi-moe"
  wandb_run_name: "rd-esi-balanced-run"
  
  # 性能优化选项
  use_amp: true
  use_checkpoint: true
  cuda_deterministic: false
  benchmark_cudnn: true
  
  # 内存优化
  optimizer_states_on_cpu: true
  save_only_last: true
  save_optimizer: true

# Evaluator configuration
evaluator:
  output_dir: "results/rd_esi_balanced_evaluation"
  device: "cuda:3"